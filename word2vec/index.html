<!DOCTYPE html><html lang="en" data-theme="light"><head><script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script><script type="text/javascript">window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"1fd8b3cd2ba70f51528f758b7",lid:"0a5bf38afd",uniqueMethods:!0})})</script><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Word2Vec: Arithmetic with Words | Research @ WDSS</title><meta name="description" content="From a young age we build familarity with the rules for adding and subtracting numbers to and from one another, but how could we go about performing arithmetic with words?"><meta name="keywords" content="lesson,word-embedding"><meta name="author" content="Warwick Data Science"><meta name="copyright" content="Warwick Data Science"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Word2Vec: Arithmetic with Words"><meta name="twitter:description" content="From a young age we build familarity with the rules for adding and subtracting numbers to and from one another, but how could we go about performing arithmetic with words?"><meta name="twitter:image" content="https://research.wdss.io/banners/word2vec.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Word2Vec: Arithmetic with Words"><meta property="og:url" content="https://research.wdss.io/word2vec/"><meta property="og:site_name" content="Research @ WDSS"><meta property="og:description" content="From a young age we build familarity with the rules for adding and subtracting numbers to and from one another, but how could we go about performing arithmetic with words?"><meta property="og:image" content="https://research.wdss.io/banners/word2vec.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode="1",t=Cookies.get("theme");if("1"==autoChangeMode){var isDarkMode=window.matchMedia("(prefers-color-scheme: dark)").matches,isLightMode=window.matchMedia("(prefers-color-scheme: light)").matches,isNotSpecified=window.matchMedia("(prefers-color-scheme: no-preference)").matches,hasNoSupport=!isDarkMode&&!isLightMode&&!isNotSpecified;if(void 0===t){if(isLightMode)activateLightMode();else if(isDarkMode)activateDarkMode();else if(isNotSpecified||hasNoSupport){console.log("You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.");var now=new Date,hour=now.getHours(),isNight=hour<6||18<=hour;isNight?activateDarkMode():activateLightMode()}}else"light"==t?activateLightMode():activateDarkMode()}else"2"==autoChangeMode?(isNight=(hour=(now=new Date).getHours())<6||18<=hour,void 0===t?isNight?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode()):"dark"==t?activateDarkMode():"light"==t&&activateLightMode();function activateDarkMode(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#000")}function activateLightMode(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#fff")}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://research.wdss.io/word2vec/"><link rel="prev" title="Money for Nothing: An Arbitrage Paradox" href="https://research.wdss.io/money-for-nothing/"><link rel="next" title="Higher or Lower: Reinventing a Classic Card Game" href="https://research.wdss.io/higher-or-lower/"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"4718922828481704",enable_page_level_ads:"true"})</script><script data-ad-client="ca-pub-4718922828481704" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"We didn't find any results for: ${query}"}},translate:void 0,copy:{success:"Copied successfully",error:"Copy failed",noSupport:"Not supported by browser"},bookmark:{message_prev:"Press",message_next:"to bookmark this page"},runtime_unit:"days",runtime:!0,copyright:void 0,ClickShowText:void 0,medium_zoom:!1,fancybox:!0,Snackbar:{bookmark:{message_prev:"Press",message_next:"to bookmark this page"},chs_to_cht:"Traditional Chinese Activated Manually",cht_to_chs:"Simplified Chinese Activated Manually",day_to_night:"Dark Mode Activated Manually",night_to_day:"Light Mode Activated Manually",bgLight:"#49b1f5",bgDark:"#2d3035",position:"bottom-left"},baiduPush:!1,highlightCopy:!0,highlightLang:!0,highlightShrink:"false",isFontAwesomeV5:!1,isPhotoFigcaption:!1}</script><script>var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isSidebar:!0}</script><noscript><style>#page-header{opacity:1}.justified-gallery img{opacity:1}</style></noscript><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Research @ WDSS" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">20</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Contents</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-number">1.</span> <span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Need-for-Word2vec"><span class="toc-number">1.1.</span> <span class="toc-text">The Need for Word2vec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deriving-the-Word-Embeddings"><span class="toc-number">1.2.</span> <span class="toc-text">Deriving the Word Embeddings</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Application"><span class="toc-number">2.</span> <span class="toc-text">Application</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Analogies"><span class="toc-number">2.1.</span> <span class="toc-text">Analogies</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Starting-With-a-Classic"><span class="toc-number">2.1.1.</span> <span class="toc-text">Starting With a Classic</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Plurals"><span class="toc-number">2.2.</span> <span class="toc-text">Plurals</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Geographical-analogies"><span class="toc-number">2.3.</span> <span class="toc-text">Geographical analogies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Opposites"><span class="toc-number">2.4.</span> <span class="toc-text">Opposites</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Vector-Sums-and-Differences"><span class="toc-number">2.5.</span> <span class="toc-text">Vector Sums and Differences</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Miscellaneous"><span class="toc-number">2.6.</span> <span class="toc-text">Miscellaneous</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec-in-the-Wild"><span class="toc-number">3.</span> <span class="toc-text">Word2Vec in the Wild</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image:url(/banners/word2vec.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Research @ WDSS</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Word2Vec: Arithmetic with Words</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-07-05 00:00:00"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-07-05</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-07-05 00:00:00"><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-07-05</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-user post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="https://www.linkedin.com/in/pratyush-ravishankar-a5391615a/" target="_blank" rel="noopener">Pratyush Ravishankar</a></span><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Science/">Computer Science</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Science/Natural-Language-Processing/">Natural Language Processing</a></span></div><div class="meta-secondline"><span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>Word count:</span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>Reading time: 12 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span></span><span class="post-meta-commentcount"><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>Comments:</span><span class="disqus-comment-count comment-count"><a href="https://research.wdss.io/word2vec/#disqus_thread"></a></span></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Unsurprisingly, human language seem intuitive to humans, but that is not the case for a computer. On the other hand, though we can easily determine whether two words share a similar meaning, if asked to give a quantitative reason as to why that is the case, we will may struggle to reach a satisfying answer. Thankfully, our shortcoming is where computers thrive, as they are remarkably good with numbers and tabular data. All this leaves us to do is to translate our problem into such a numeric system. Word embedding is a technique to take language and transform it into this computer-friendly format so we can take advantage of computing power for solving natural language processing tasks.</p><h2 id="Background">Background</h2><h3 id="The-Need-for-Word2vec">The Need for Word2vec</h3><p>Initial attempts at solving this problem before word embeddings included one-hot encoding. The essence of this process is to represent our text as a sequence of vectors in a space with number of dimensions equal to the number of words in our corpus. We assign each word its own dimension so that each word vector is orthogonal to the others. Technical details aside, this isn’t practical to model anything on the scale of human languages, as the vector dimensions would be the size of the language (and as a results, storage and computational requirements would be astronomical). More importantly, however, this doesn’t capture any sort of similarity between two words (since ever vector is mutually orthogonal to the others). As far as this method is concerned, the words ‘dolphin’ and ‘neoliberal’ are equally similar to ‘shark’. Word2vec aims to solve this problem by providing word embedding which take into account relations between words. In essence, word2vec provides a canvas (albeit a strange multi-dimensional one) where any possible word in the language could lie, and plots points on this canvas for each word in our corpus. How close any two points on this canvas lie (captured mathematically by the cosine distance) should therefore correspond to how likely humans are to describe the respresented words as “similar”.</p><p><img src="/" class="lazyload" data-src="/images/word2vec/word2vec_6_0.png" alt=""></p><h3 id="Deriving-the-Word-Embeddings">Deriving the Word Embeddings</h3><p>To derive each word embedding, the word2vec model is usually trained using a method called Skipgram with Negative Sampling (SGNS). Essentially, a large corpus (typically billions of words) is fed to the model, and an <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span>-sized sliding window is used to capture the words that lie either side of each word in the corpus to determine each word’s context. The context for each word is then used to determine the word’s embedding vector, with the negative sampling process controlling the rate at which these weights are updated to reduce computation time and produce a more robust result. Because words with a similar context usually have closely-linked meanings, such words will end up having similar embedding vectors too.</p><p><img src="/" class="lazyload" data-src="/images/word2vec/sliding_window.png" alt="The sliding window used in word2vec training"><br>Take the above diagram as an example. On iteration <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85396em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span>, ‘fox’ and ‘bear’ have similar contexts, so will end up with relatively close embedding vectors. After many adjustments each time they are found in the corpus, their vectors will provide an increasingly accurate represention of the “fox” and “bear” relation—types of animals.</p><h2 id="Application">Application</h2><div class="note info"><p>The following examples are derived from a word2vec model trained on the <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">Google News dataset</a>, featuring over 100 billion words taken from various news articles. The trained model is stored an object called <code>model</code> which we can query for results.</p></div><p>Once we have trained a word embedding using word2vec, we can apply it in many different ways to extract the relationships between words in the corpus.</p><p>For example, we can find the similarity between words based on their cosine distance in the vector space.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'queen'</span>, <span class="string">'throne'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.45448625
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'queen'</span>, <span class="string">'forklift'</span>)</span><br></pre></td></tr></table></figure><pre><code>-0.030027825
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'Queen'</span>, <span class="string">'Bowie'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.20833209
</code></pre><div class="note warning"><p>Since we are measuring similarity using the cosine distance, values will range from -1 to 1. Words with a similarity near 1 are likely to be extremely similar, words with a similarity of 0 have little in common, and words with similarity near -1 <em>should</em> be opposites (though we’ll later see that this doesn’t always work)</p></div><p>As expected, the word ‘forklift’ is relatively distinct from ‘queen’, especially when compared to ‘throne’. What’s fascinating, however, is that multiple facets of the word ‘queen’ are captured; we see that ‘Bowie’ is also relatively close to ‘Queen’ due to the word’s relation to the iconic rock band.</p><p>Naturally, with vectors come mathematical operations, and the real power of word2vec starts to emerge. Vector differences are the crux behind <em>analogies</em>, a concept best explained through examples…</p><h3 id="Analogies">Analogies</h3><h4 id="Starting-With-a-Classic">Starting With a Classic</h4><p>The most infamous example of the use of word2vec is answering the question, “Man is to woman, as king is to…what?”. As we can see, word2vec takes this puzzle in it’s stride.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># king + (woman - man) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'king'</span>, <span class="string">'woman'</span>], negative=[<span class="string">'man'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>queen (0.712)
</code></pre><p>This example is rather intuitive; the female version of the male title ‘king’ is ‘queen’ and so this is the natural choice to complete the analogy. To get word2vec to return this result, we have to phrase the question in the language of arthimetic; that is, <code>king + (woman - man)</code>. In other words, we are taking the word ‘king’ and asking what the corresponding word would be if added the difference between ‘woman’ and ‘man’. This may seem unituitive—why can’t we just add ‘woman’ to ‘king’? The reason for this is that the word ‘king’ already has ‘man’ as a component of its vector representation. Therefore, if we simply added ‘woman’ without first subtracting ‘man’ we end up with components of both ‘woman’ and ‘man’ which confuses the model, leaving us with nonsensical results.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Invalid approach: king + woman = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'king'</span>, <span class="string">'woman'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>man (0.663)
</code></pre><h3 id="Plurals">Plurals</h3><p>We can use this system of analogy solving for finding the singular and plural forms of words. With a rather mundane example such as <code>gloves + (bike - bikes)</code>, it’s not unsurprising the model returns ‘glove’; it could simply be obtained from deciphering that the pattern is removing the trailing ‘s’—hardly groundbreaking. But when talking about irregular plurals, the required task to output the derived word shifts from spotting a simple pattern to seemingly needing a human-like understanding of the structure and complexities of the English language. Never-the-less, word2vec is up for the challenge.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># foot + (cacti - cactus) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'foot'</span>, <span class="string">'cacti'</span>], negative=[<span class="string">'cactus'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>feet (0.568)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># child + (sheep - sheep) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'child'</span>, <span class="string">'sheep'</span>], negative=[<span class="string">'sheep'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>children (0.726)
</code></pre><p>Here, ‘sheep’ is both the singular and the plural, meaning the result of the word arithmetic is still ‘child’. But since ‘child’ is such a similar word to ‘children’, word2vec still manages to come out with the correct answer.</p><h3 id="Geographical-analogies">Geographical analogies</h3><p>We can use analogies to find cities.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Portugal + (Moscow - Russia) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Portugal'</span>, <span class="string">'Moscow'</span>], negative=[<span class="string">'Russia'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Lisbon (0.655)
</code></pre><p>Or we can flip things around to find what country a city resides in.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Delhi + (Spain - Barcelona) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Delhi'</span>, <span class="string">'Spain'</span>], negative=[<span class="string">'Barcelona'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>India (0.626)
</code></pre><p>Finally, we can go one step up and find the geographic regions of countries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cambodia + (Africa - Egypt) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Cambodia'</span>, <span class="string">'Africa'</span>], negative=[<span class="string">'Egypt'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Southeast_Asia (0.566)
</code></pre><p>The geographic intelligence of word2vec isn’t limited to the form of analogy. Here we see an example in which we perform straight addition.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iran + war = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Iran'</span>, <span class="string">'war'</span>],topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Iraq (0.683)
Islamic_republic (0.671)
Syria (0.653)
</code></pre><p>This example shows how much geographic and political complexity is captured in the model. ‘Iraq’ and ‘Islamic_republic’ are most likely referencing the Iran-Iraq war. On the other hand Iraq and Syria, are both war-stricken countries near Iran, which could easily explain this relation.</p><h3 id="Opposites">Opposites</h3><p>When a word has a clear opposite, we can use analogy to find it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># high + (big - small)</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'high'</span>, <span class="string">'big'</span>], negative=[<span class="string">'small'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>low (0.448)
</code></pre><p>Note however that we can’t just negate a word to find its opposite or we obtain gibberish in return.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -high</span></span><br><span class="line">print_similar(model.most_similar(negative=[<span class="string">'high'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>----------_-----------------------------------------------_GS## (0.321)
</code></pre><p>The reason for this is that an opposite word in a vector space has to be opposite in every way. Even though we would say that ‘high’ and ‘low’ are opposites, they do in fact have components in common, such as how they both represent heights. For that reason the model stubbornly ignores words that are opposites in the way we intend, and instead, tries to find the word that is most dissimilar to ‘high’, resulting in some strange garble of characters.</p><h3 id="Vector-Sums-and-Differences">Vector Sums and Differences</h3><p>As hinted at before, word2vec can solve problems far more general than analogies. Here we look at some examples of generic vector sums and differences.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># death + water = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'death'</span>, <span class="string">'water'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>drowning (0.556)
drowing (0.545)
scalding_bath (0.526)
</code></pre><div class="note info"><p>It appears that the second most similar term has picked up on a common typo of ‘drowning’. The joys of real world data…</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># death + knife = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'death'</span>, <span class="string">'knife'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>kitchen_knife (0.644)
stabbing (0.637)
murder (0.634)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># girlfriend - love = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'girlfriend'</span>], negative=[<span class="string">'love'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>ex_girlfriend (0.517)
fiancee (0.479)
estranged_wife (0.476)
</code></pre><p>The second result is rather strange. If you have a theory of where this relation might have come from, make sure to comment below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colleague + love = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'colleage'</span>, <span class="string">'love'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>loved (0.580)
friend (0.551)
pal (0.542)
</code></pre><p>With the last example we can see a shortcoming of the word2vec model. It appears that ‘love’ has a much stronger vector representation than ‘collegue’; that is, the term captures more complexity, which makes sense. For this reason, the term ‘love’ can overpower the sum so that a word similar to ‘love’, ‘loved’ can be returned as highly similar even though it doesn’t relate much to the word ‘collegue’. Despite this, the other two preditions are strong.</p><h3 id="Miscellaneous">Miscellaneous</h3><p>To wrap up our examples, we will look at some miscellaneous analogies involving people and places.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obama + (Russia - USA) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Obama'</span>, <span class="string">'Russia'</span>], negative=[<span class="string">'USA'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Medvedev (0.674)
Putin (0.647)
Kremlin (0.617)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UK + (Hitler - Germany) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'UK'</span>, <span class="string">'Hitler'</span>], negative=[<span class="string">'Germany'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Tony_Blair (0.522)
Oliver_Cromwell (0.509)
Maggie_Thatcher (0.506)
</code></pre><p>Make of the above what you will…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apple + (Gates - Microsoft) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Apple'</span>, <span class="string">'Gates'</span>], negative=[<span class="string">'Microsoft'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Steve_Jobs (0.523)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Victoria Beckham + (Barack Obama - Michelle) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Victoria_Beckham'</span>, <span class="string">'Barack_Obama'</span>], negative=[<span class="string">'Michelle'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>David_Beckham (0.528)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Manchester + (Anfield - Liverpool) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Manchester'</span>, <span class="string">'Anfield'</span>], negative=[<span class="string">'Liverpool'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Old_Trafford (0.765)
</code></pre><h2 id="Word2Vec-in-the-Wild">Word2Vec in the Wild</h2><p>Above, we have seen some fairly isolated applications of the word2vec model, but that is not to say that there are not wider reaching use cases. For example, word2vec is often a key step in the production of sentiment analysis models (See: <a href="https://youtu.be/l40-JFn6F9M?t=1845" target="_blank" rel="noopener">a WDSS virtual talk on the use of sentiment analysis for predicting presidential approval</a>), recommender systems, and chat bots. Aside from these ecommerce-centric examples, word2vec has also flurished in scientific applications such as BioNLP, which have utilised word embeddings for advancements in knowledge.</p><p>Hopefully, through these examples, the potential power of Word2Vec has been made clear. Thank you reading.</p></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://www.linkedin.com/in/pratyush-ravishankar-a5391615a/" target="_blank" rel="noopener">Pratyush Ravishankar</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Permalink: </span><span class="post-copyright-info"><a href="https://research.wdss.io/word2vec/">https://research.wdss.io/word2vec/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless otherwise specified.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/lesson/">lesson</a><a class="post-meta__tags" href="/tags/word-embedding/">word-embedding</a></div><div class="post_share"><div class="social-share" data-image="/banners/coin-flip.jpg" data-sites="facebook,twitter,linkedin"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/money-for-nothing/"><img class="prev_cover lazyload" data-src="/banners/money-for-nothing.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Money for Nothing: An Arbitrage Paradox</div></div></a></div><div class="next-post pull_right"><a href="/higher-or-lower/"><img class="next_cover lazyload" data-src="/banners/higher-or-lower.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Higher or Lower: Reinventing a Classic Card Game</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommended</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/gibrats-law/" title="Gibrat's Law: The Central Limit Theorem's Forgotten Twin"><img class="relatedPosts_cover lazyload" data-src="/banners/gibrats-law.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-10</div><div class="relatedPosts_title">Gibrat's Law: The Central Limit Theorem's Forgotten Twin</div></div></a></div><div class="relatedPosts_item"><a href="/money-for-nothing/" title="Money for Nothing: An Arbitrage Paradox"><img class="relatedPosts_cover lazyload" data-src="/banners/money-for-nothing.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-23</div><div class="relatedPosts_title">Money for Nothing: An Arbitrage Paradox</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="disqus_thread"></div><script>var disqus_config=function(){this.page.url="https://research.wdss.io/word2vec/",this.page.identifier="word2vec/",this.page.title="Word2Vec: Arithmetic with Words"};!function(){var e=document,t=e.createElement("script");t.src="https://wdss-research-blog.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}()</script><script>function getDisqusCount(){var s=document,t=s.createElement("script");t.src="https://wdss-research-blog.disqus.com/count.js",t.id="dsq-count-scr",(s.head||s.body).appendChild(t)}window.addEventListener("load",getDisqusCount,!1)</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Warwick Data Science</div><div class="framework-info"><span>Powered By </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Comments"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49b1f5">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function(){$("span.katex-display").wrap('<div class="katex-wrap"></div>')})</script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="/js/search/local-search.js"></script></body></html>