<!DOCTYPE html><html lang="en" data-theme="light"><head><script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script><script type="text/javascript">window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"1fd8b3cd2ba70f51528f758b7",lid:"0a5bf38afd",uniqueMethods:!0})})</script><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Oh Hell! - Reinforcement Learning for solving card games | Research @ WDSS</title><meta name="description" content="In this article we are going to plays cards. Well, not exactly ... with the power of reinforcement learning we are going to train intelligent agents that will play the game for us!"><meta name="keywords" content="reinforcement-learning,card-games,agent-learning"><meta name="author" content="Warwick Data Science"><meta name="copyright" content="Warwick Data Science"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Oh Hell! - Reinforcement Learning for solving card games"><meta name="twitter:description" content="In this article we are going to plays cards. Well, not exactly ... with the power of reinforcement learning we are going to train intelligent agents that will play the game for us!"><meta name="twitter:image" content="https://research.wdss.io/banners/cards.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Oh Hell! - Reinforcement Learning for solving card games"><meta property="og:url" content="https://research.wdss.io/oh-hell/"><meta property="og:site_name" content="Research @ WDSS"><meta property="og:description" content="In this article we are going to plays cards. Well, not exactly ... with the power of reinforcement learning we are going to train intelligent agents that will play the game for us!"><meta property="og:image" content="https://research.wdss.io/banners/cards.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode="1",t=Cookies.get("theme");if("1"==autoChangeMode){var isDarkMode=window.matchMedia("(prefers-color-scheme: dark)").matches,isLightMode=window.matchMedia("(prefers-color-scheme: light)").matches,isNotSpecified=window.matchMedia("(prefers-color-scheme: no-preference)").matches,hasNoSupport=!isDarkMode&&!isLightMode&&!isNotSpecified;if(void 0===t){if(isLightMode)activateLightMode();else if(isDarkMode)activateDarkMode();else if(isNotSpecified||hasNoSupport){console.log("You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.");var now=new Date,hour=now.getHours(),isNight=hour<6||18<=hour;isNight?activateDarkMode():activateLightMode()}}else"light"==t?activateLightMode():activateDarkMode()}else"2"==autoChangeMode?(isNight=(hour=(now=new Date).getHours())<6||18<=hour,void 0===t?isNight?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode()):"dark"==t?activateDarkMode():"light"==t&&activateLightMode();function activateDarkMode(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#000")}function activateLightMode(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#fff")}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://research.wdss.io/oh-hell/"><link rel="prev" title="One-Hit Wonders" href="https://research.wdss.io/one-hit-wonders/"><link rel="next" title="Reinventing Test and Trace: A Bayesian Approach For Modelling SARS-CoV-2 Setting-Specific Transmission" href="https://research.wdss.io/track-and-trace/"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"4718922828481704",enable_page_level_ads:"true"})</script><script data-ad-client="ca-pub-4718922828481704" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"We didn't find any results for: ${query}"}},translate:void 0,copy:{success:"Copied successfully",error:"Copy failed",noSupport:"Not supported by browser"},bookmark:{message_prev:"Press",message_next:"to bookmark this page"},runtime_unit:"days",runtime:!0,copyright:void 0,ClickShowText:void 0,medium_zoom:!1,fancybox:!0,Snackbar:{bookmark:{message_prev:"Press",message_next:"to bookmark this page"},chs_to_cht:"Traditional Chinese Activated Manually",cht_to_chs:"Simplified Chinese Activated Manually",day_to_night:"Dark Mode Activated Manually",night_to_day:"Light Mode Activated Manually",bgLight:"#49b1f5",bgDark:"#2d3035",position:"bottom-left"},baiduPush:!1,highlightCopy:!0,highlightLang:!0,highlightShrink:"false",isFontAwesomeV5:!1,isPhotoFigcaption:!1}</script><script>var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isSidebar:!0}</script><noscript><style>#page-header{opacity:1}.justified-gallery img{opacity:1}</style></noscript><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Research @ WDSS" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">37</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">23</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Contents</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Approach"><span class="toc-number">2.</span> <span class="toc-text">The Approach</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Programming-the-Environment"><span class="toc-number">3.</span> <span class="toc-text">Programming the Environment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Extracting-the-input-state-from-the-environment"><span class="toc-number">4.</span> <span class="toc-text">Extracting the input state from the environment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Opponents-for-the-agent-and-NFSP"><span class="toc-number">5.</span> <span class="toc-text">Opponents for the agent and NFSP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Choosing-an-RL-algorithm"><span class="toc-number">6.</span> <span class="toc-text">Choosing an RL algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Neural-Network-Architecture"><span class="toc-number">7.</span> <span class="toc-text">The Neural Network Architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation-of-Algorithm"><span class="toc-number">8.</span> <span class="toc-text">Implementation of Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Initial-Results"><span class="toc-number">9.</span> <span class="toc-text">Initial Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Final-Results"><span class="toc-number">10.</span> <span class="toc-text">Final Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">11.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image:url(/banners/cards.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Research @ WDSS</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Oh Hell! - Reinforcement Learning for solving card games</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2021-12-04 00:00:00"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2021-12-04</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2021-12-04 00:00:00"><i class="fa fa-history" aria-hidden="true"></i> Updated 2021-12-04</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><a class="post-meta__categories" href="https://www.linkedin.com/in/albert-nyarko-agyei-6ab804188/" target="_blank" rel="noopener">Albert Nyarko-Agyei</a><span>, </span><a class="post-meta__categories" href="https://www.linkedin.com/in/alexandru-pascu/" target="_blank" rel="noopener">Alexandru Pascru</a><span>, </span><a class="post-meta__categories" href="https://www.linkedin.com/in/ronaldcvek/" target="_blank" rel="noopener">Ron Cvek</a></span><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Science/">Computer Science</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Science/Games/">Games</a></span></div><div class="meta-secondline"><span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>Word count:</span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>Reading time: 11 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span></span><span class="post-meta-commentcount"><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>Comments:</span><span class="disqus-comment-count comment-count"><a href="https://research.wdss.io/oh-hell/#disqus_thread"></a></span></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><div class="note info"><p>This post is the corresponding write-up for a WDSS project in which Albert Nyarko-Agyei, Alexandru Pascru and Ron Cvek applied reinforcement learning to better understand the mechanics of a British card game—<i>Oh Hell</i>. The rules and an online version of the game can be found <a href="https://cardgames.io/ohhell/" target="_blank" rel="noopener">here</a>. Special credits to Henry Charlesworth for providing valuable insights. The main author of the project and the blog-post is Albert Nyarko-Agyei.</p></div><h2 id="Introduction">Introduction</h2><p>The idea for this project started when I was introduced to the game, <i>Oh Hell</i>, by one of the contributors to the project. We played a variation where each player is given 7 cards and then the players take it in turn to predict how many rounds they will win. Interestingly, predicting correctly gives you extra points or ‘tricks.’ So, if many games are being played, a good prediction strategy is needed.</p><p>After bidding, the round starts with the player to the left of the dealer playing a card from their hand. Each player then has to play a card from the same suit if they have one, otherwise any card from their hand is permissible. Once everyone has played a card, the player who played the highest card from the first suit wins the round, gets a trick added to their total, and starts the next round. Two caveats to these rules are:</p><ol><li><p>Along with dealing cards for the game, a trump card is drawn and if a card from with the same suit is played, it will beat every other suit.</p></li><li><p>The number of tricks bid by the last bidder can not make the total number of tricks bid equal to the number of cards dealt to each player.</p></li></ol><p>Now that we have the rules down, some different strategies might have come across your mind. Horde your trump cards till the end? Bid a low number of tricks? Bid a high number of tricks because you’re confident? What is the perfect strategy to this game? Turning to AI and Reinforcement Learning seemed a logical way to answer these questions.</p><p><strong>Note</strong>: There are two main themes in this project, the programming of agents/models, and the theory behind why certain methods work; feel free to pick which sections interest you.</p><h2 id="The-Approach">The Approach</h2><p><strong>Reinforcement learning</strong> is a branch of Machine Learning that deals with the <strong>actions</strong> an <strong>agent(s)</strong> should take based on it’s <strong>observation</strong> of an <strong>environment</strong> in order to maximise a <strong>reward</strong>. If our environment is a simple <i>Oh Hell</i> game, we can formulate our problem as designing an agent to predict what action will give the most reward, given that our environment is in a particular <strong>state</strong> and thus what action to take.</p><p><img src="/" class="lazyload" data-src="/images/oh-hell/rlvisualisation.jpg" alt="Main-Components-of-RL" title="Main Components of RL"></p><h2 id="Programming-the-Environment">Programming the Environment</h2><p>The first step was to implement a game of <i>Oh Hell</i> in Python. Most games studied under reinforcement learning follow a similar structure, and some build up to a class representing the environment the agent can train in. If your custom environment inherits from an existing base environment, you can use high level implementations of RL algorithms that require that base. For this project <a href="https://github.com/openai/gym" target="_blank" rel="noopener">OpenAI’s gym</a> environment class was used. This base class has the abstract methods, step (take a single action) and reset (restart the environment). These methods are called by the final training algorithm.</p><p>The rules of <i>Oh Hell</i> use elements of <i>Poker</i> and <i>Uno</i> which have already been implemented countless times. The module <a href="https://github.com/datamllab/rlcard" target="_blank" rel="noopener">RLCARD</a> by MIT has a set of already implemented environments for some card games, including <i>Poker</i> and <i>Uno</i>. Also the structure of RLCARD’s environments is very clean, so the base of this project used a lot of their methods. At the time of development, however, the environments in RLCARD were only for two-player games and did not have a PPO implementation (will cover more) so it had to be extended.</p><p>Below is the structure of the different classes in the code. Composition was used instead of inheritance; for instance, the Round class creates instances of players, a dealer, and a judger and their interaction is controlled within the Round class. A round in this code represents every player taking a single action, making a bid or playing a single card. A game represents the start of the bidding to the last card being played.</p><p><a href="https://github.com/LeLingu/OhHellProject/tree/main/rlohhell/games/ohhell" target="_blank" rel="noopener"><img src="/" class="lazyload" data-src="/images/oh-hell/gamecode.jpg" alt="Structure-of-code" title="Structure of Code"></a></p><h2 id="Extracting-the-input-state-from-the-environment">Extracting the input state from the environment</h2><p>Now, the agent needs to be able to ‘see’ the environment and then make predictions about what action to take. This process can be viewed as a function, so neural networks come in to try and approximate the function that gives the best predictions. If we encode the useful information in the game, the neural network should be able to draw out a strategy.</p><p>There are a few approaches to this particular section, one could encode almost everything in the environment and make it very easy for the neural network to pick out the optimal actions - or you could give the neural network only the basic information. The first method is a useful debugging tool since an agent with all the information about it’s environment should have no excuse not to learn. The latter method takes less time to program but would take longer to train and would not necessarily give the best model because some useful information could be neglected.</p><p>The encoding for the model at the time of this blog is below, and a detailed explanation of the <a href="https://github.com/LeLingu/OhHellProject/blob/main/rlohhell/envs/ohhell.py" target="_blank" rel="noopener">encodings</a> can be found on the github.</p><p><img src="/" class="lazyload" data-src="/images/oh-hell/currentencoding.jpg" alt="Encoding"></p><h2 id="Opponents-for-the-agent-and-NFSP">Opponents for the agent and NFSP</h2><p>After extending the environment to a four-player game, what the agent would train against came into question. The approach that was taken in this project was inspired by the idea of <strong>Neural Fictitious Self-Play</strong> developed by David Silver and Johannes Heinrich, leading researchers in RL. At it’s core, the idea is to train the agent against itself. Initially, a basic model was trained against random agents, then a new model was trained against three versions of the last agent. The theory is to repeat this process each time until the new agent beats the old agents. Below is a snippet of the code from the environment that uses this idea:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''This section plays the game with the dummy agents until it is time for the training </span></span><br><span class="line"><span class="string">agent to play then the agent's observation of the environment can be extracted'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> self.game.players[self.game.current_player].name != <span class="string">'Training'</span>:</span><br><span class="line">    <span class="comment"># The state is extracted from the current game</span></span><br><span class="line">    current_obs = self._extract_state(self.game.get_state(self.game.current_player))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The observation is used in the trained model to predict an action</span></span><br><span class="line">    fictitious_action, _ = self.trained_model.predict(current_obs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The action is taken </span></span><br><span class="line">    _, _ = self.game.step(self._decode_action(fictitious_action))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This section was used for training against random agents</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># a = random.choice(self.game.get_legal_actions())</span></span><br><span class="line">    <span class="comment"># _,_ = self.game.step(a)</span></span><br></pre></td></tr></table></figure><h2 id="Choosing-an-RL-algorithm">Choosing an RL algorithm</h2><p>Coming back to the function we need to approximate, which is formally called the <strong>policy</strong>, there are a few different methods to choose from, however, they can all be categorised into two groups. Methods that generate training data (experience) from the same policy that is being evaluated are called <strong>on-policy</strong>. Methods that have two separate policies, one for generating experience and another for evaluation, are called <strong>off-policy</strong>.</p><p>The method we chose for this project was on-policy <strong>Proximal Policy Optimization</strong> or <strong>PPO</strong> by OpenAI. To quote their website directly, RL algorithms have a lot of ‘moving parts’ which makes the training of agents very unstable. PPO, however, uses a clipped objective function which reduces changes to the current policy so that the performance doesn’t suddenly decline whilst training. A detailed explanation of the objective function can be found <a href="https://openai.com/blog/openai-baselines-ppo/" target="_blank" rel="noopener">here</a>.</p><h2 id="The-Neural-Network-Architecture">The Neural Network Architecture</h2><p>Adding on to our understanding of how to approximate the earlier function, PPO outputs the actions for the agent to take in the form of <strong>logits</strong> whilst it also outputs the <strong>state value</strong>. The logits are unnormalised predictions of the actions to take, the state value is the expected reward the agent will get from the current state if it follows the current policy. This additional evaluation of the state is why PPO is sometimes called an Actor-Critic algorithm.</p><p>With this in mind, shared layers in the <i>Oh Hell</i> agent meant the agent was unable to learn until the actor and critic were spilt into two different networks. The reason for this is not clear, but a common internal conflict that agents face in RL is choosing actions that maximize immediate reward vs long-term rewards or the crucial <strong>Exploration vs Exploitation Dilemma</strong>.</p><p>The structure of the neural network that generated the best model is below, with details of how many neurons where used at each level.</p><p><img src="/" class="lazyload" data-src="/images/oh-hell/neuralnetwork.jpg" alt="Neural-network-architecture"></p><h2 id="Implementation-of-Algorithm">Implementation of Algorithm</h2><p>To run the PPO algorithm on the custom environment, the package <a href="https://github.com/DLR-RM/stable-baselines3" target="_blank" rel="noopener">stable-baselines3</a> was used. It has a PPO implementation that allows you to customise the layers on a neural network and offers tensorboard support so that you can visualise training results. The entire code that generates the neural network model is found <a href="https://github.com/LeLingu/OhHellProject/blob/main/custom_ppo/running_model.py" target="_blank" rel="noopener">here</a> with explanations of what each section does.</p><p>Below are some of the metrics that were tracked by tensorboard during training.</p><p><img src="/" class="lazyload" data-src="/images/oh-hell/tensorboard.jpeg" alt="Viewing-tensorboard-integration"></p><h2 id="Initial-Results">Initial Results</h2><p>The initial models struggled to correctly select which of the 63 actions were actually possible at any given point. In the game design, ineligible actions are replaced with a random action - so these initial models did not improve on what a random agent would have done.</p><p>At this stage, one could implement a so-called ‘mask’ over the unavailable actions which sets the logits of these actions to 0 or change the reward signal to the agent so that it is ‘punished’ everytime an unavailable action is selected. The latter approach is less coercive and is what was implemented. However, noticeably, it still leaves the agent with the ability to choose unavailable actions.</p><h2 id="Final-Results">Final Results</h2><p>The best trained model so far can not answer the original question of the best possible strategy for playing <em>Oh Hell</em>. The policy, however, shows a large improvement from a random agent; it will sometimes execute flawless runs, predicting a number of tricks, winning that number, and getting the extra bonus of 10 tricks and with no ineligible actions selected. This model will select an average of just under 2 unavailable actions per game which is a massive improvement from the original 10 out of 11.</p><p><img src="/" class="lazyload" data-src="/images/oh-hell/gameplay1.jpg" alt="gameplay"></p><h2 id="Conclusion">Conclusion</h2><p>This project answered interesting questions of how RL agents are trained and this particular approach of using neural networks, Deep RL being only one of the methods in RL. If there was more time, a Monte Carlo Tree Search could hypothetically be implemented on top of this agent to improve it’s performance or a completely custom and more granular PPO implementation along with a mask.</p></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Authors: </span><span class="post-copyright-info"><a class="post-meta__categories" href="https://www.linkedin.com/in/albert-nyarko-agyei-6ab804188/" target="_blank" rel="noopener">Albert Nyarko-Agyei</a><span>, </span><a class="post-meta__categories" href="https://www.linkedin.com/in/alexandru-pascu/" target="_blank" rel="noopener">Alexandru Pascru</a><span>, </span><a class="post-meta__categories" href="https://www.linkedin.com/in/ronaldcvek/" target="_blank" rel="noopener">Ron Cvek</a></span></div><div class="post-copyright__author"><span class="post-copyright-meta">Editor: </span><span class="post-copyright-info"><a href="https://www.linkedin.com/in/keeley-ruane-6aab4219b/" target="_blank" rel="noopener">Keeley Ruane</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Permalink: </span><span class="post-copyright-info"><a href="https://research.wdss.io/oh-hell/">https://research.wdss.io/oh-hell/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless otherwise specified.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/reinforcement-learning/">reinforcement-learning</a><a class="post-meta__tags" href="/tags/card-games/">card-games</a><a class="post-meta__tags" href="/tags/agent-learning/">agent-learning</a></div><div class="post_share"><div class="social-share" data-image="/banners/voting.png" data-sites="facebook,twitter,linkedin"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/one-hit-wonders/"><img class="prev_cover lazyload" data-src="/banners/one-hit-wonders.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">One-Hit Wonders</div></div></a></div><div class="next-post pull_right"><a href="/track-and-trace/"><img class="next_cover lazyload" data-src="/banners/covid.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Reinventing Test and Trace: A Bayesian Approach For Modelling SARS-CoV-2 Setting-Specific Transmission</div></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="disqus_thread"></div><script>var disqus_config=function(){this.page.url="https://research.wdss.io/oh-hell/",this.page.identifier="oh-hell/",this.page.title="Oh Hell! - Reinforcement Learning for solving card games"};!function(){var e=document,t=e.createElement("script");t.src="https://wdss-research-blog.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}()</script><script>function getDisqusCount(){var s=document,t=s.createElement("script");t.src="https://wdss-research-blog.disqus.com/count.js",t.id="dsq-count-scr",(s.head||s.body).appendChild(t)}window.addEventListener("load",getDisqusCount,!1)</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Warwick Data Science</div><div class="framework-info"><span>Powered By </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Comments"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49b1f5">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function(){$("span.katex-display").wrap('<div class="katex-wrap"></div>')})</script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="/js/search/local-search.js"></script></body></html>