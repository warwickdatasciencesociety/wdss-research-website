<!DOCTYPE html><html lang="en" data-theme="light"><head><script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script><script type="text/javascript">window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"1fd8b3cd2ba70f51528f758b7",lid:"0a5bf38afd",uniqueMethods:!0})})</script><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>A Data-Driven Dive into UK Party Conference Leaders' Speeches | Research @ WDSS</title><meta name="description" content="Wordclouds, sentiment, and scrabble scores. In this post, we analyse and visualisation a collection Leaders' speeches, using traditional and not-so-traditional text analysis techniques."><meta name="keywords" content="web-scraping,visualization,data-analysis"><meta name="author" content="Warwick Data Science"><meta name="copyright" content="Warwick Data Science"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="A Data-Driven Dive into UK Party Conference Leaders' Speeches"><meta name="twitter:description" content="Wordclouds, sentiment, and scrabble scores. In this post, we analyse and visualisation a collection Leaders' speeches, using traditional and not-so-traditional text analysis techniques."><meta name="twitter:image" content="https://research.wdss.io/banners/leaders-speeches.jpg"><meta property="og:type" content="article"><meta property="og:title" content="A Data-Driven Dive into UK Party Conference Leaders' Speeches"><meta property="og:url" content="https://research.wdss.io/leaders-speeches/"><meta property="og:site_name" content="Research @ WDSS"><meta property="og:description" content="Wordclouds, sentiment, and scrabble scores. In this post, we analyse and visualisation a collection Leaders' speeches, using traditional and not-so-traditional text analysis techniques."><meta property="og:image" content="https://research.wdss.io/banners/leaders-speeches.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode="1",t=Cookies.get("theme");if("1"==autoChangeMode){var isDarkMode=window.matchMedia("(prefers-color-scheme: dark)").matches,isLightMode=window.matchMedia("(prefers-color-scheme: light)").matches,isNotSpecified=window.matchMedia("(prefers-color-scheme: no-preference)").matches,hasNoSupport=!isDarkMode&&!isLightMode&&!isNotSpecified;if(void 0===t){if(isLightMode)activateLightMode();else if(isDarkMode)activateDarkMode();else if(isNotSpecified||hasNoSupport){console.log("You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.");var now=new Date,hour=now.getHours(),isNight=hour<6||18<=hour;isNight?activateDarkMode():activateLightMode()}}else"light"==t?activateLightMode():activateDarkMode()}else"2"==autoChangeMode?(isNight=(hour=(now=new Date).getHours())<6||18<=hour,void 0===t?isNight?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode()):"dark"==t?activateDarkMode():"light"==t&&activateLightMode();function activateDarkMode(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#000")}function activateLightMode(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#fff")}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://research.wdss.io/leaders-speeches/"><link rel="prev" title="Simulating Planetary Motion with Python" href="https://research.wdss.io/planetary-motion/"><link rel="next" title="Horsing Around: Problem-Solving Using Monte Carlo Markov Chains" href="https://research.wdss.io/horsing-around/"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"4718922828481704",enable_page_level_ads:"true"})</script><script data-ad-client="ca-pub-4718922828481704" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"We didn't find any results for: ${query}"}},translate:void 0,copy:{success:"Copied successfully",error:"Copy failed",noSupport:"Not supported by browser"},bookmark:{message_prev:"Press",message_next:"to bookmark this page"},runtime_unit:"days",runtime:!0,copyright:void 0,ClickShowText:void 0,medium_zoom:!1,fancybox:!0,Snackbar:{bookmark:{message_prev:"Press",message_next:"to bookmark this page"},chs_to_cht:"Traditional Chinese Activated Manually",cht_to_chs:"Simplified Chinese Activated Manually",day_to_night:"Dark Mode Activated Manually",night_to_day:"Light Mode Activated Manually",bgLight:"#49b1f5",bgDark:"#2d3035",position:"bottom-left"},baiduPush:!1,highlightCopy:!0,highlightLang:!0,highlightShrink:"false",isFontAwesomeV5:!1,isPhotoFigcaption:!1}</script><script>var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isSidebar:!0}</script><noscript><style>#page-header{opacity:1}.justified-gallery img{opacity:1}</style></noscript><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Research @ WDSS" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">15</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">17</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Contents</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#data-background"><span class="toc-number">1.</span> <span class="toc-text">Data Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#text-analysis"><span class="toc-number">2.</span> <span class="toc-text">Text Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#analysing-sentiment"><span class="toc-number">2.1.</span> <span class="toc-text">Analysing Sentiment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#term-frequency-and-zipfs-law"><span class="toc-number">2.2.</span> <span class="toc-text">Term Frequency and Zipf’s Law</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-idf-analysis"><span class="toc-number">2.3.</span> <span class="toc-text">TF-IDF Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#complexity-consideration"><span class="toc-number">2.4.</span> <span class="toc-text">Complexity Consideration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-different-way-of-deciding-elections"><span class="toc-number">2.5.</span> <span class="toc-text">A Different Way of Deciding Elections?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#takeaways"><span class="toc-number">3.</span> <span class="toc-text">Takeaways</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#appendix-summary-table-of-all-speech-metrics"><span class="toc-number">4.</span> <span class="toc-text">Appendix: Summary Table of All Speech Metrics</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image:url(/banners/leaders-speeches.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Research @ WDSS</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">A Data-Driven Dive into UK Party Conference Leaders' Speeches</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2021-01-04 00:00:00"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2021-01-04</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2021-01-04 00:00:00"><i class="fa fa-history" aria-hidden="true"></i> Updated 2021-01-04</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-user post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="https://www.linkedin.com/in/ewan-yeaxlee-7b13a9181/" target="_blank" rel="noopener">Ewan Yeaxlee</a></span><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Science/">Computer Science</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Science/Natural-Language-Processing/">Natural Language Processing</a><span>, </span><a class="post-meta__categories" href="/categories/Social-Sciences/">Social Sciences</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Social-Sciences/Politics/">Politics</a></span></div><div class="meta-secondline"><span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>Word count:</span><span class="word-count">2.5k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>Reading time: 15 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span></span><span class="post-meta-commentcount"><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>Comments:</span><span class="disqus-comment-count comment-count"><a href="https://research.wdss.io/leaders-speeches/#disqus_thread"></a></span></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Party conferences are a mainstay in British politics whereby politicians, party members and affiliated people descend on a chosen city in order to set the party agenda, raise funds and attempt to get a soundbite into the mainstream media. The hallmark of these conferences are the Leaders’ speeches, where the current head of the party aims to appeal to their party base or even attract some new voters through media coverage.</p><h2 id="data-background"><a class="markdownIt-Anchor" href="#data-background"></a> Data Background</h2><p>This analysis would not have been possible without the transcripts provided by <a href="http://www.britishpoliticalspeech.org/speech-archive.htm" target="_blank" rel="noopener">British Political Speech</a>. They describe themselves as “an online archive of British political speech and a place for the discussion, analysis, and critical appreciation of political rhetoric” and produce speeches dating back to 1895.</p><p>For my study, I aim to observe the nuances of party conference leadership speeches from 2010 to 2018. These dates were chosen as they coincide with a change in the British political landscape, following the 2010 election whilst still providing us with enough data to conduct meaningful analysis. For this study, I will only observe the three ‘mainstay’ political parties: the Conservatives, the Labour Party and the Liberal Democrats.</p><p>Upon importing and tidying the data, we can observe the 5 most used words within the speeches.</p><table><caption>A tibble: 5 × 2</caption><thead><tr><th scope="col">Word</th><th scope="col">Count</th></tr><tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;int&gt;</th></tr></thead><tbody><tr><td>the</td><td>8777</td></tr><tr><td>to</td><td>5279</td></tr><tr><td>and</td><td>5033</td></tr><tr><td>of</td><td>3898</td></tr><tr><td>a</td><td>3470</td></tr></tbody></table><p>There are no surprises here. In fact, the top 5 most used words here are from the top 6 most used words in the English language according to the <a href="https://enacademic.com/dic.nsf/enwiki/2822326" target="_blank" rel="noopener">Oxford English Corpus</a>, a text corpus comprising over 2 billion words.</p><p>Carrying on our analysis with these common words would create a dull analysis, so to counteract this, we will temporarily remove them. We do this using the <code>tidytext</code> package in R, which contains a comprehensive list of <a href="https://rdrr.io/cran/tidytext/man/stop_words.html" target="_blank" rel="noopener">stop words</a>. These are common words in the English language which would add nothing to certain parts of our analysis if they were to be included. A separate dataframe was created to store the non-stopwords which totalled 56,989 words, meaning that 105,883 words were removed.</p><div class="note info"><p>It is worth noting, that there is no definitive list of stopwords. Instead, different words would be considered stopwords depending on the context, although we have just used a generic list for simplicity. We will see later in the post, a more nuanced way of handling uninformative words called TF-IDF.</p></div><p>We can now observe the most used 5 non-stopwords.</p><table><caption>A tibble: 5 × 2</caption><thead><tr><th scope="col">Word</th><th scope="col">Count</th></tr><tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;int&gt;</th></tr></thead><tbody><tr><td>people</td><td>1207</td></tr><tr><td>country</td><td>684</td></tr><tr><td>government</td><td>598</td></tr><tr><td>party</td><td>577</td></tr><tr><td>britain</td><td>568</td></tr></tbody></table><p>This is more like what we would have expected the vocabulary of a Leader’s speech to look like.</p><p>We can go further and visualise the set of each party’s 100 most commonly used non-stopwords through wordclouds. This is done below in each party’s traditional colour. (Blue for Conservative, Red for Labour, Yellow for Liberal Democrat).</p><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_16_0.png" alt=""></p><p>We can see that the words identified to be most common before appear most often in these wordclouds too (denoted by their large size). The only visible differences are the party’s names, particularly visible for both Labour and the Liberal Democrats. Here we see the obvious flaws in word clouds, they barely allow us to observe any differences between the parties and provide no numerical insight. We will aim to address this weakness later with different methods.</p><p>Before we dive into some more detailed text analysis, we could have a quick exploration of the word count for each Leader’s speech.</p><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_20_0.png" alt=""></p><p>From this, we can see that Ed Miliband can be quite the rambler at times.</p><h2 id="text-analysis"><a class="markdownIt-Anchor" href="#text-analysis"></a> Text Analysis</h2><h3 id="analysing-sentiment"><a class="markdownIt-Anchor" href="#analysing-sentiment"></a> Analysing Sentiment</h3><p>Basic counts and summaries are great, but with modern data science techniques, we can go much further. For example, we can infer the sentiment (loosely, how positive or negative in tone) each speech is.</p><p>We do this by referencing the contents of each speech against the AFINN lexicon, created by Finn Arup Neilson. This lexicon assigns an integer value from -5 to 5 to a vast number English words with negative numbers indicating negative sentiment and positive numbers indicating positive sentiment. Here we list a random word for each sentiment value.</p><table><caption>A grouped_df: 11 × 2</caption><thead><tr><th scope="col">Word</th><th scope="col">Value</th></tr><tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr></thead><tbody><tr><td>son-of-a-bitch</td><td>-5</td></tr><tr><td>fraudulence</td><td>-4</td></tr><tr><td>lunatics</td><td>-3</td></tr><tr><td>lethargy</td><td>-2</td></tr><tr><td>manipulation</td><td>-1</td></tr><tr><td>some kind</td><td>0</td></tr><tr><td>cool</td><td>1</td></tr><tr><td>courtesy</td><td>2</td></tr><tr><td>cheery</td><td>3</td></tr><tr><td>winner</td><td>4</td></tr><tr><td>superb</td><td>5</td></tr></tbody></table><p>We can then take an average of the sentiments over all word in each speech and visualise it to see the trend of speech sentiment over time. This is conducted on the dataset with stopwords included, otherwise it could distort the sentiment (though note that most stopwords have a neutral sentiment).</p><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_28_0.png" alt=""></p><p>As we can see, the speeches are overwhelmingly positive, with the only negative score being Jeremy Corbyn’s 2018 speech to the Labour Party conference. Other notable values include David Cameron’s consistency between 2010 and 2015 for the Conservative Party and the significant jump in positivity when Theresa May took over the Conservative Leadership in 2016.</p><h3 id="term-frequency-and-zipfs-law"><a class="markdownIt-Anchor" href="#term-frequency-and-zipfs-law"></a> Term Frequency and Zipf’s Law</h3><p>We have seen that raw word counts on their own aren’t particularly useful. One flaw of many is that longer texts will naturally have higher word counts for all words. Instead, a more useful metric is how often a certain word (also called a term) appears as a proportion of all words. This is known as <em>term frequency</em> and defined as</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Term Frequency</mtext><mo>=</mo><mfrac><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">{</mo><mtext>Occurrences of Term</mtext><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">{</mo><mtext>Occurrences of All Words</mtext><mo stretchy="false">}</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Term Frequency} = \frac{\#\{\text{Occurrences of Term}\}}{\#\{\text{Occurrences of All Words}\}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord text"><span class="mord">Term Frequency</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">{</span><span class="mord text"><span class="mord">Occurrences of All Words</span></span><span class="mclose">}</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">{</span><span class="mord text"><span class="mord">Occurrences of Term</span></span><span class="mclose">}</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>So that we can compare across parties, we will look at term frequencies as a proportion of the occurrences of each term in all speeches by the party the term came from. We start by looking at the distribution of term frequencies for each party.</p><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_35_0.png" alt=""></p><div class="note warning"><p>It should be noted that there are longer tails for these graphs that have not been shown. Instead, we have truncated the the really popular words such as ‘the’, ‘and’ and ‘to’ to make it easier to see the main body of the plot.</p></div><p>The plots all display a similar distribution for each party with many ‘rare’ words and fewer popular words.</p><p>It turns out that these long-tailed distributions are common in almost every occurrence of natural language. In fact, George Zipf, a 20th century American linguist created <em>Zipf’s law</em>. This formalises the above observation, stating that the frequency that a word appears in a text is inversely proportional to its rank.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>Term Frequency</mtext><mo>∝</mo><mfrac><mn>1</mn><mtext>Rank</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{Term Frequency} \propto \frac{1}{\text{Rank}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord text"><span class="mord">Term Frequency</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord">Rank</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>Put simply, the most frequent word will appear at twice the rate of the second most frequent word and at three times that of the third most frequent word.</p><p>Zipf’s law is largely accurate for many natural languages, including English (though as always, there are exceptions). For example, in the Brown Corpus of American English text, which contains slightly over 1 million words: ‘the’ appears the most times at ~70000 times, ‘of’ the second most at ~36000 times and ‘and’ the third most at ~29000, as would be roughly expected according to Zipf’s law.</p><p>We can attempt to visualise this law for our own text by plotting rank on the x-axis and term frequency on the y-axis, both on log scales.</p><div class="note info"><p><strong>Why the logs?</strong><br>By definition, if two values <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">x</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span></span></span></span> are inversely proportional, then we can find a constant <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">a</span></span></span></span> such that <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mi>a</mi><mi>x</mi></mfrac></mrow><annotation encoding="application/x-tex">y = \frac{a}{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.040392em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.695392em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>. Taking logarithms and rearranging gives <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log(y) = log(a) - log(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>. In other words, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">x</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span></span></span></span> are inversely proportional if and only if their logarithms lie on a straight line with a negative slope.</p></div><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_41_0.png" alt=""></p><p>We can see that all three parties have similar text structures largely obey Zipf’s Law. That said, we can see that our curve deviates from a straight line at the lower rank tail, suggesting that the most popular words in the speeches are being used more often than they would in a natural language. Additionally, we would expect a slope of approximately <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">−</span><span class="mord">1</span></span></span></span>; by fitting a linear model (shown in grey), we obtain a coefficient which is close to this value.</p><h3 id="tf-idf-analysis"><a class="markdownIt-Anchor" href="#tf-idf-analysis"></a> TF-IDF Analysis</h3><p>We’ve seen that we can use a list of stop words to filter our data to leave only meaningful words. However, this list is fixed and not linked to our data in any way. We’ve already seen that ‘people’ is used very commonly in our speeches and so doesn’t provide that meaningful of an insight to us. Could construct a value that helped us to see the relative frequency of a term among our speeches, in order to see how important a word is to a specific speech compared to the others?</p><p>We can indeed. In fact the work has already been done for us in the form of a value value called the TF-IDF. It is calculated by multiplying the term frequency (TF) from earlier by a new value called the inverse document frequency (IDF).</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mi>F</mi><mo>⋅</mo><mrow><mi>I</mi><mi>D</mi><mi>F</mi></mrow><mo>=</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">{</mo><mtext>Occurrences of Term</mtext><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">{</mo><mtext>Occurrences of All Words</mtext><mo stretchy="false">}</mo></mrow></mfrac><mo fence="true">)</mo></mrow><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">{</mo><mtext>Documents</mtext><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">{</mo><mtext>Documents Containing Term</mtext><mo stretchy="false">}</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">TF\cdot{IDF} = \left(\frac{\#\{\text{Occurrences of Term}\}}{\#\{\text{Occurrences of All Words}\}}\right) \cdot \log \left(\frac{\#\{\text{Documents}\}}{\#\{\text{Documents Containing Term}\}}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">T</span><span class="mord mathdefault" style="margin-right:.13889em">F</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="mord mathdefault" style="margin-right:.02778em">D</span><span class="mord mathdefault" style="margin-right:.13889em">F</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-.95003em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">{</span><span class="mord text"><span class="mord">Occurrences of All Words</span></span><span class="mclose">}</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">{</span><span class="mord text"><span class="mord">Occurrences of Term</span></span><span class="mclose">}</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-.95003em"></span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">{</span><span class="mord text"><span class="mord">Documents Containing Term</span></span><span class="mclose">}</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">{</span><span class="mord text"><span class="mord">Documents</span></span><span class="mclose">}</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size3">)</span></span></span></span></span></span></span></p><p>Loosely speaking, TF-IDF asks two questions:</p><ul><li>Is the specific term used more than expected in a given speech?</li><li>Is it rare for a speech to contain a the specific term?<br>If the answer to both of these questions is “yes”, then TF-IDF is large, an the term is considered to be relatively important.</li></ul><p>We can calculate the TF-IDF score for each word in each speech before using these to find the most ‘important’ word in each speech.</p><table><caption>A grouped_df: 9 × 4</caption><thead><tr><th scope="col">year</th><th scope="col">Conservative</th><th scope="col">Labour</th><th scope="col">Liberal Democrat</th></tr><tr><th scope="col">&lt;int&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th></tr></thead><tbody><tr><td>2010</td><td>harry</td><td>recognises</td><td>plural</td></tr><tr><td>2011</td><td>euro</td><td>bargain</td><td>barons</td></tr><tr><td>2012</td><td>rise</td><td>succeeded</td><td>maurice</td></tr><tr><td>2013</td><td>finish</td><td>race</td><td>liberal</td></tr><tr><td>2014</td><td>40p</td><td>ethic</td><td>dems</td></tr><tr><td>2015</td><td>extremism</td><td>kinder</td><td>liberal</td></tr><tr><td>2016</td><td>plays</td><td>migrants</td><td>brexit</td></tr><tr><td>2017</td><td>dream</td><td>grenfell</td><td>brexit</td></tr><tr><td>2018</td><td>proposal</td><td>palestinian</td><td>brexit</td></tr></tbody></table><p>We obtain some interesting results here. For example, it’s clear to see the Liberal Democrats’ sharp pivot to a anti-Brexit strategy following the referendum of 2016. Or how in 2014, the Conservatives announced their plan to increase the 40% income tax threshold (known as the 40p tax rate). We also see Jeremy Corbyn’s plan for a ‘kinder’ politics emerge in his first conference speech as leader in 2015, alongside the Grenfell Tower disaster mentioned in 2017.</p><p>The names such as ‘Harry’ and ‘Maurice’ that crop up here were intriguing at first glance. These were in reference to ‘Harry Beckough’ and ‘Maurice Reeves’, who were, respectively, a longstanding Conservative member and a furniture shop owner whose premises was burned to the ground during the London riots.</p><h3 id="complexity-consideration"><a class="markdownIt-Anchor" href="#complexity-consideration"></a> Complexity Consideration</h3><p>There are a number of ways that we can observe the complexity of a text, or in this case a speech. For this piece we choose the average number of syllables per word. The data for this was taken from the <code>quanteda</code> package and we can visualise the results as so.</p><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_54_0.png" alt=""></p><p>We can see profound variations between different leaders in this plot. Ed Miliband and David Cameron, the leaders of Labour and the Conservatives who gave speeches between 2010-2014 and 2010-2015, respectively, had a much lower complexity than the most recent leaders such as Jeremy Corbyn of Labour and Vince Cable of the Liberal Democrats, who together count for the top 6 most complex speeches.</p><div class="note success"><p>We used mean syllable count in this piece as a metric for speech complexity as it is simple for a layperson to understand. That said, there are many more subtle and interesting complexity measures available through <code>quanteda</code>, such as the Flesch–Kincaid readability score.</p></div><h3 id="a-different-way-of-deciding-elections"><a class="markdownIt-Anchor" href="#a-different-way-of-deciding-elections"></a> A Different Way of Deciding Elections?</h3><p>The <a href="https://www.electoral-reform.org.uk/voting-systems/types-of-voting-system/first-past-the-post/" target="_blank" rel="noopener">First Past the Post system</a> is often bemoaned in the UK as being unsuitable for modern-day politics. Now, it is not my place to comment on this system but if pushed to suggest another system, the aforementioned <code>Quanteda</code> package does give us another option…</p><p>We can calculate the mean scrabble score per word of the leader’s party conference speech each year! First let us observe the most impressive efforts that the politicians managed:</p><table><caption>A tibble: 5 × 5</caption><thead><tr><th scope="col">Party</th><th scope="col">Year</th><th scope="col">Leader</th><th scope="col">Word</th><th scope="col">Scrabble Score</th></tr><tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr></thead><tbody><tr><td>Conservative</td><td>2018</td><td>Theresa May</td><td>czechoslovakia</td><td>37</td></tr><tr><td>Liberal Democrat</td><td>2013</td><td>Nick Clegg</td><td>unequivocally</td><td>30</td></tr><tr><td>Labour</td><td>2017</td><td>Jeremy Corbyn</td><td>overwhelmingly</td><td>29</td></tr><tr><td>Labour</td><td>2017</td><td>Jeremy Corbyn</td><td>democratization</td><td>29</td></tr><tr><td>Labour</td><td>2015</td><td>Jeremy Corbyn</td><td>fizzing</td><td>29</td></tr></tbody></table><p>Theresa May managed an incredible score of 37 in 2018 with ‘Czechoslovakia’ but this would of course be disqualified for being a proper noun. As a result, Nick Clegg holds the record with 30 points scored for ‘unequivocally’! We can also visualise the mean score per word as follows.</p><p><img src="/" class="lazyload" data-src="/images/leaders-speeches/leaders-speeches_60_0.png" alt=""></p><p>As we can see, the Conservatives, who have been in power since 2010 would not win a single year should it be decided by Scrabble. In fact, the Liberal Democrats would win 6 out of the 9 years we have studied with Labour, under Jeremy Corbyn, taking the other 3 years—I’m sure both parties would be happy with that in hindsight!</p><p>Just in case anyone was under any illusion, of course mean Scrabble score is a poor way of deciding elections and I am not endorsing its use—at the very least, a game of Pictionary would be more appropriate…</p><h2 id="takeaways"><a class="markdownIt-Anchor" href="#takeaways"></a> Takeaways</h2><p>With that, I end my brief incursion into British political speeches. While I have barely begun to scratch the surface of Natural Language Processing (NLP) methods, I hope that I have shown the power of the ways that these techniques can be used to summarise large pieces of text through sentiment, TF-IDF and syllable complexity.</p><p>I had minimal experience with NLP methods upon embarking on this project and would like to thank WDSS (in particular, Janique Krasnowska) for supporting me until completion. I feel like I’ve learned a lot and certainly furthered my knowledge and experience. I would suggest anyone who would like to conduct some data science studies outside their degree looks out for research opportunites with WDSS and seizes them with both hands—I will certainly be looking out for more chances!</p><h2 id="appendix-summary-table-of-all-speech-metrics"><a class="markdownIt-Anchor" href="#appendix-summary-table-of-all-speech-metrics"></a> Appendix: Summary Table of All Speech Metrics</h2><table><caption>A tibble: 27 × 8</caption><thead><tr><th scope="col">Party</th><th scope="col">Year</th><th scope="col">Leader</th><th scope="col">Number of Words</th><th scope="col">Mean Sentiment</th><th scope="col">Top TF-IDF Word</th><th scope="col">Mean Word Syllables</th><th scope="col">Mean Scrabble Score</th></tr><tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr></thead><tbody><tr><td>Conservative</td><td>2010</td><td>David Cameron</td><td>6247</td><td>0.38866397</td><td>harry</td><td>1.439020</td><td>7.435761</td></tr><tr><td>Conservative</td><td>2011</td><td>David Cameron</td><td>6132</td><td>0.40983607</td><td>euro</td><td>1.450808</td><td>7.538034</td></tr><tr><td>Conservative</td><td>2012</td><td>David Cameron</td><td>6070</td><td>0.47016706</td><td>rise</td><td>1.403032</td><td>7.367087</td></tr><tr><td>Conservative</td><td>2013</td><td>David Cameron</td><td>5917</td><td>0.45477387</td><td>finish</td><td>1.400913</td><td>7.401018</td></tr><tr><td>Conservative</td><td>2014</td><td>David Cameron</td><td>6104</td><td>0.49638554</td><td>40p</td><td>1.378480</td><td>7.304463</td></tr><tr><td>Conservative</td><td>2015</td><td>David Cameron</td><td>6676</td><td>0.45546559</td><td>extremism</td><td>1.443745</td><td>7.463356</td></tr><tr><td>Conservative</td><td>2016</td><td>Theresa May</td><td>7187</td><td>0.88888889</td><td>plays</td><td>1.476137</td><td>7.568521</td></tr><tr><td>Conservative</td><td>2017</td><td>Theresa May</td><td>7113</td><td>0.63288719</td><td>dream</td><td>1.485799</td><td>7.432207</td></tr><tr><td>Conservative</td><td>2018</td><td>Theresa May</td><td>7120</td><td>0.54709419</td><td>proposal</td><td>1.483296</td><td>7.571831</td></tr><tr><td>Labour</td><td>2010</td><td>Ed Miliband</td><td>6168</td><td>0.40265487</td><td>recognises</td><td>1.480791</td><td>7.348780</td></tr><tr><td>Labour</td><td>2011</td><td>Ed Miliband</td><td>5891</td><td>0.56207675</td><td>bargain</td><td>1.393482</td><td>7.288363</td></tr><tr><td>Labour</td><td>2012</td><td>Ed Miliband</td><td>7390</td><td>0.40898345</td><td>succeeded</td><td>1.388167</td><td>7.134647</td></tr><tr><td>Labour</td><td>2013</td><td>Ed Miliband</td><td>7954</td><td>0.74186992</td><td>race</td><td>1.370724</td><td>7.068717</td></tr><tr><td>Labour</td><td>2014</td><td>Ed Miliband</td><td>5697</td><td>0.79874214</td><td>ethic</td><td>1.437664</td><td>7.322393</td></tr><tr><td>Labour</td><td>2015</td><td>Jeremy Corbyn</td><td>7178</td><td>0.47313692</td><td>kinder</td><td>1.530257</td><td>7.609025</td></tr><tr><td>Labour</td><td>2016</td><td>Jeremy Corbyn</td><td>5894</td><td>0.43455497</td><td>migrants</td><td>1.569805</td><td>7.809402</td></tr><tr><td>Labour</td><td>2017</td><td>Jeremy Corbyn</td><td>5965</td><td>0.08439898</td><td>grenfell</td><td>1.585235</td><td>7.864050</td></tr><tr><td>Labour</td><td>2018</td><td>Jeremy Corbyn</td><td>5703</td><td>-0.04136253</td><td>palestinian</td><td>1.564188</td><td>7.706432</td></tr><tr><td>Liberal Democrat</td><td>2010</td><td>Nick Clegg</td><td>4354</td><td>0.16060606</td><td>plural</td><td>1.457425</td><td>7.571065</td></tr><tr><td>Liberal Democrat</td><td>2011</td><td>Nick Clegg</td><td>4257</td><td>0.06571429</td><td>barons</td><td>1.471583</td><td>7.604947</td></tr><tr><td>Liberal Democrat</td><td>2012</td><td>Nick Clegg</td><td>4328</td><td>0.33333333</td><td>maurice</td><td>1.468837</td><td>7.439471</td></tr><tr><td>Liberal Democrat</td><td>2013</td><td>Nick Clegg</td><td>5927</td><td>0.49206349</td><td>liberal</td><td>1.475379</td><td>7.538670</td></tr><tr><td>Liberal Democrat</td><td>2014</td><td>Nick Clegg</td><td>6241</td><td>0.33682008</td><td>dems</td><td>1.494074</td><td>7.640180</td></tr><tr><td>Liberal Democrat</td><td>2015</td><td>Tim Farron</td><td>5804</td><td>0.12616822</td><td>liberal</td><td>1.462427</td><td>7.327490</td></tr><tr><td>Liberal Democrat</td><td>2016</td><td>Tim Farron</td><td>6178</td><td>0.21428571</td><td>brexit</td><td>1.454840</td><td>7.391953</td></tr><tr><td>Liberal Democrat</td><td>2017</td><td>Vince Cable</td><td>5139</td><td>0.20505618</td><td>brexit</td><td>1.574762</td><td>7.818679</td></tr><tr><td>Liberal Democrat</td><td>2018</td><td>Vince Cable</td><td>4364</td><td>0.08626198</td><td>brexit</td><td>1.544255</td><td>7.863584</td></tr></tbody></table></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://www.linkedin.com/in/ewan-yeaxlee-7b13a9181/" target="_blank" rel="noopener">Ewan Yeaxlee</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Permalink: </span><span class="post-copyright-info"><a href="https://research.wdss.io/leaders-speeches/">https://research.wdss.io/leaders-speeches/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless otherwise specified.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/web-scraping/">web-scraping</a><a class="post-meta__tags" href="/tags/visualization/">visualization</a><a class="post-meta__tags" href="/tags/data-analysis/">data-analysis</a></div><div class="post_share"><div class="social-share" data-image="/banners/planetary-motion.jpg" data-sites="facebook,twitter,linkedin"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/planetary-motion/"><img class="prev_cover lazyload" data-src="/banners/planetary-motion.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Simulating Planetary Motion with Python</div></div></a></div><div class="next-post pull_right"><a href="/horsing-around/"><img class="next_cover lazyload" data-src="/banners/horsing-around.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Horsing Around: Problem-Solving Using Monte Carlo Markov Chains</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommended</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/league-of-its-own/" title="A League of Its Own: Rethinking University League Tables"><img class="relatedPosts_cover lazyload" data-src="/banners/league-of-its-own.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-25</div><div class="relatedPosts_title">A League of Its Own: Rethinking University League Tables</div></div></a></div><div class="relatedPosts_item"><a href="/urban-cities/" title="Urban Cities: A History Told By Data"><img class="relatedPosts_cover lazyload" data-src="/banners/urban-cities.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-23</div><div class="relatedPosts_title">Urban Cities: A History Told By Data</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="disqus_thread"></div><script>var disqus_config=function(){this.page.url="https://research.wdss.io/leaders-speeches/",this.page.identifier="leaders-speeches/",this.page.title="A Data-Driven Dive into UK Party Conference Leaders' Speeches"};!function(){var e=document,s=e.createElement("script");s.src="https://wdss-research-blog.disqus.com/embed.js",s.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(s)}()</script><script>function getDisqusCount(){var s=document,t=s.createElement("script");t.src="https://wdss-research-blog.disqus.com/count.js",t.id="dsq-count-scr",(s.head||s.body).appendChild(t)}window.addEventListener("load",getDisqusCount,!1)</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Warwick Data Science</div><div class="framework-info"><span>Powered By </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Comments"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49b1f5">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function(){$("span.katex-display").wrap('<div class="katex-wrap"></div>')})</script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="/js/search/local-search.js"></script></body></html>